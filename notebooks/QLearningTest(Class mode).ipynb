{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Q-Learning implementation for PymGrid (class mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don'f forget to add you own path to sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/alami/OneDrive/Documents/Reinforcement Learning/MicrogridManagement/pymgrid_v2/pymgrid-master/RL_evaluation')\n",
    "import tqdm\n",
    "sys.path.append('../')\n",
    "from pymgrid import MicrogridGenerator as MG\n",
    "#from RL_Algo.QLearning import *\n",
    "#from RL_Algo.RandomPolicy import *\n",
    "import numpy as np\n",
    "import time\n",
    "#from MicroGridEnv import *\n",
    "import random \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class MicroGridEnv:  Markov Decision Process modeling the microgrid dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MicroGridEnv():\n",
    "    \"\"\"\n",
    "    Markov Decision Process associated to the microgrid.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            microgrid: microgrid, mandatory\n",
    "                The controlled microgrid.\n",
    "            random_seed: int, optional\n",
    "                Seed to be used to generate the needed random numbers to size microgrids.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, microgrid, seed = 0):\n",
    "        # Set seed\n",
    "        np.random.seed(seed)\n",
    "        # Microgrid\n",
    "        self.mg = microgrid\n",
    "        # State space\n",
    "        self.observation_space = self.states()\n",
    "        # Action space\n",
    "        self.action_space = [0,1,2,3]\n",
    "        # Number of states\n",
    "        self.Ns = len(self.observation_space)\n",
    "        # Number of actions\n",
    "        self.Na = len(self.action_space)\n",
    "        self.state = None\n",
    "        self.round = None\n",
    "\n",
    "        # Start the first round\n",
    "        self.reset()\n",
    "\n",
    "    # Transition function\n",
    "    def transition(self):\n",
    "        net_load = round(self.mg.load - self.mg.pv)\n",
    "        soc = round(self.mg.battery.soc,1)\n",
    "        s_ = (net_load, soc)  # next state\n",
    "        return s_\n",
    "            \n",
    "    # Reward function\n",
    "    def reward(self):\n",
    "        return -self.mg.get_cost() - self.mg.penalty(0.5)\n",
    "    \n",
    "    def step(self, action):\n",
    "        control_dict = self.get_action(action)\n",
    "        _ = self.mg.run(control_dict)\n",
    "        reward = self.reward()\n",
    "        s_ = self.transition()\n",
    "        self.state = s_\n",
    "        done = self.round == self.mg.horizon\n",
    "        self.round += 1\n",
    "        return s_, reward, done, {}\n",
    "    \n",
    "        \n",
    "    def reset(self):\n",
    "        self.round = 1\n",
    "        # Reseting microgrid\n",
    "        self.mg.reset()\n",
    "        # Building first state\n",
    "        net_load = round(self.mg.load - self.mg.pv)\n",
    "        soc = round(self.mg.battery.soc,1)\n",
    "        self.state = (net_load, soc)\n",
    "\n",
    "    # Building the observations_space from the forecast time series\n",
    "    def states(self):\n",
    "        observation_space = []\n",
    "        mg = self.mg\n",
    "        net_load = mg.forecast_load() - mg.forecast_pv()\n",
    "        for i in range(int(net_load.min()-1),int(net_load.max()+2)):\n",
    "            for j in np.arange(round(mg.battery.soc_min,1),round(mg.battery.soc_max+0.1,1),0.1):    \n",
    "                j = round(j,1)\n",
    "                observation_space.append((i,j)) \n",
    "        return observation_space\n",
    "    \n",
    "    # Mapping between action and the control_dict\n",
    "    def get_action(self, action):\n",
    "        \"\"\"\n",
    "        :param action: current action\n",
    "        :return: control_dict : dicco of controls\n",
    "        \"\"\"\n",
    "        mg = self.mg\n",
    "        pv = mg.pv\n",
    "        load = mg.load\n",
    "        net_load = load - pv\n",
    "        capa_to_charge = mg.battery.capa_to_charge\n",
    "        p_charge_max = mg.battery.p_charge_max\n",
    "        p_charge = max(0,min(-net_load, capa_to_charge, p_charge_max))\n",
    "        \n",
    "        capa_to_discharge = mg.battery.capa_to_discharge\n",
    "        p_discharge_max = mg.battery.p_discharge_max\n",
    "        p_discharge = max(0,min(net_load, capa_to_discharge, p_discharge_max))\n",
    "    \n",
    "        control_dict = {'pv_consummed': min(pv,load),\n",
    "                        'battery_charge': 0,\n",
    "                        'battery_discharge': 0,\n",
    "                        'grid_import': 0,\n",
    "                        'grid_export':0\n",
    "                               }\n",
    "        if action == 0:\n",
    "            control_dict['battery_charge'] = p_charge*(p_charge > 0) + net_load*(p_charge <=0)  \n",
    "            control_dict['grid_export'] = max(0,pv - min(pv,load) - p_charge)\n",
    "        \n",
    "        elif action == 1:\n",
    "            control_dict['battery_discharge'] = p_discharge*(p_discharge > 0) + net_load*(p_discharge <=0)  \n",
    "            control_dict['grid_import'] = max(0,load - min(pv,load) - p_discharge)\n",
    "        \n",
    "        elif action == 2:\n",
    "            control_dict['grid_import'] = abs(net_load)\n",
    "            \n",
    "        elif action == 3:\n",
    "            control_dict['grid_export'] = abs(net_load)\n",
    "            \n",
    "        return control_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    \"\"\"\n",
    "    Implementation of Q-learning algorithm with epsilon-greedy exploration\n",
    "\n",
    "    If learning_rate is None; alpha(x,a) = 1/max(1, N(s,a))**alpha\n",
    "    \n",
    "    Parameters\n",
    "        ----------\n",
    "            env: environment modeled by an MDP.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, gamma, alpha=0.6, learning_rate=None, min_learning_rate=0.01, epsilon=0.99, epsilon_decay=0.9995,\n",
    "                 epsilon_min=0.25, seed=42):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.Q = np.zeros((env.Ns, env.Na)) # Numpy Array. States are indexed with integers\n",
    "        self.Nsa = np.zeros((env.Ns, env.Na))\n",
    "        self.state = env.reset()\n",
    "        self.RS = np.random.RandomState(seed)\n",
    "\n",
    "    def get_delta(self, r, x_i, a, y_i):\n",
    "        \"\"\"\n",
    "        :param r: reward\n",
    "        :param x_i: index of current state\n",
    "        :param a: current action\n",
    "        :param y_i: index of next state\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        max_q_y_a = self.Q[y_i, :].max()\n",
    "        q_x_a = self.Q[x_i, a]\n",
    "\n",
    "        return r + self.gamma*max_q_y_a - q_x_a\n",
    "\n",
    "    def get_learning_rate(self, x, a):\n",
    "        \"\"\"\n",
    "        :param x: current state\n",
    "        :param a: current action\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        if self.learning_rate is None:\n",
    "            return max(1.0/max(1.0, self.Nsa[x, a])**self.alpha, self.min_learning_rate)\n",
    "        else:\n",
    "            return max(self.learning_rate, self.min_learning_rate)\n",
    "\n",
    "    def get_action(self, x_i):\n",
    "        \"\"\"\n",
    "        :param x_i: index of current state\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        if self.RS.uniform(0, 1) < self.epsilon:\n",
    "            # explore\n",
    "            return random.choice(self.env.action_space)\n",
    "        else:\n",
    "            # exploit\n",
    "            a = self.Q[x_i, :].argmax()\n",
    "            return a\n",
    "\n",
    "    def step(self):\n",
    "        # Current state\n",
    "        x = self.env.state\n",
    "        x_i = self.env.observation_space.index(x)\n",
    "        # Choose action\n",
    "        a = self.get_action(x_i)\n",
    "\n",
    "        # Learning rate\n",
    "        alpha = self.get_learning_rate(x_i, a)\n",
    "\n",
    "        # Take step\n",
    "        y, reward, done, info = self.env.step(a) \n",
    "        r = reward\n",
    "        y_i = self.env.observation_space.index(y)\n",
    "        delta = self.get_delta(r, x_i, a, y_i)\n",
    "\n",
    "        # Update\n",
    "        self.Q[x_i, a] = self.Q[x_i, a] + alpha*delta\n",
    "\n",
    "        self.Nsa[x_i, a] += 1\n",
    "        \n",
    "        if done:\n",
    "            # print(x, observation, reward)\n",
    "            self.epsilon = max(self.epsilon*self.epsilon_decay, self.epsilon_min)\n",
    "            self.env.reset()\n",
    "        return done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launching the Q Learning algorithm on the created microgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "     Training a Q Learning Policy            \n",
      "----------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "              Q values & greedy policy         \n",
      "-----------------------------------------------\n",
      "\n",
      "Q values (Q Learning);\n",
      " [[     0.              0.              0.              0.        ]\n",
      " [     0.              0.              0.              0.        ]\n",
      " [  -187.5             0.              0.              0.        ]\n",
      " ...\n",
      " [     0.              0.              0.              0.        ]\n",
      " [-15838.4937909    -411.47901131      0.              0.        ]\n",
      " [-14401.76539584   -411.47901131   -781.81012149 -27431.93408732]]\n",
      "\n",
      "Policy (Q Learning):  [0 0 1 ... 0 2 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mg_generator = MG.MicrogridGenerator(nb_microgrid=1)\n",
    "mg_generator.generate_microgrid(verbose = False)\n",
    "microgrid = mg_generator.microgrids[0]\n",
    "\n",
    "# Itiniation of a MicroGridEnv\n",
    "env = MicroGridEnv(microgrid = microgrid)\n",
    "\n",
    "gamma = 0.9 # Discount factor\n",
    "n_episodes = 10 # Number of episodes\n",
    "\n",
    "\"\"\"\n",
    "Q LEARNING\n",
    "\"\"\"\n",
    "# Initiation of a qlearning object\n",
    "qlearning = QLearning(env, gamma=gamma, epsilon = 0.2)\n",
    "\n",
    "print(\"----------------------------------------------\")\n",
    "print(\"     Training a Q Learning Policy            \")\n",
    "print(\"----------------------------------------------\")\n",
    "\n",
    "outer = tqdm.tqdm(total = n_episodes, position=0)\n",
    "\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    outer.update(1)\n",
    "    done = False\n",
    "    while not done:\n",
    "        done = qlearning.step()\n",
    "\n",
    "        \n",
    "print(\"-----------------------------------------------\")        \n",
    "print(\"              Q values & greedy policy         \")\n",
    "print(\"-----------------------------------------------\")\n",
    "\n",
    "print(\"\\nQ values (Q Learning);\\n\", qlearning.Q)\n",
    "print(\"\\nPolicy (Q Learning): \", np.argmax(qlearning.Q, axis=1))\n",
    "       \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
